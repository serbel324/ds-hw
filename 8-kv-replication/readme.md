# Распределенное key-value хранилище с репликацией

В этой задаче вам предстоит усовершенствовать key-value хранилище из предыдущего ДЗ, обеспечив отказоустойчивое хранение данных путем их репликации. Теперь каждая запись должна храниться не на одном узле, а на трех. Таким образом, при выходе из строя 1-2 узлов, данные не должны теряться. При этом остается шардинг данных, то есть в системе из 6 узлов каждый будет хранить примерно половину данных. Чтобы не создавать зависимость от предыдущего ДЗ, будем использовать простейший (и не самый эффективный) способ шардинга данных и обойдемся без перебалансировки шардов. Тем самым, сконцентрируемся теперь на реализации репликации и связанных с этим проблемах. 

Пусть в нашей системе есть _N_ узлов с идентификаторами _[0, N)_. Тогда за хранение ключа _K_ будет отвечать узел с идентификатором _P_ = _hash(K) mod N_. А, так как мы хотим реплицировать данные на трех узлах, то добавим к этому узлу еще два узла, следующие за ним в порядке их идентификаторов. Например, если _N_=6 и для некоторого ключа _P_=4, то за хранение этого ключа будут отвечать узлы _{4, 5, 0}_. Далее будем называть эти узлы и хранимые ими копии данных _репликами_. 

Для реализации репликации будем использовать подход без лидера, описанный в [статье про Amazon Dynamo](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf). Операции на чтение (GET) и запись (PUT, DELETE) выполняются с кворумами, размеры которых указывает клиент при отправке операции. Запрос на выполнение операции может быть сделан локально с любого узла. Узел рассылает операцию по репликам и, при получении кворума ответов, сообщает результат операции локальному клиенту.

Мы хотим сделать наше хранилище высокодоступным и уметь успешно обрабатывать запросы в присутствии отказов узлов и сети. Если из-за отказов для успешного выполнения операции недостаточно живых реплик (не набирается нужный кворум), то в качестве резервных реплик должны использоваться другие доступные узлы. Если отказавшая реплика стала снова доступна, то резервная должна передать ей записанные данные. Этот подход (sloppy quorum + hinted handoff) также описан в статье про Dynamo.

Для простоты будем считать, что отказы временные, и в случае обнаружения отказа не требуется делать перебалансировку шардов или переназначать реплики между узлами. Кроме того, вашей реализации не требуется поддерживать добавление и удаление узлов из системы, в тестах состав узлов фиксирован.

В силу описанного подхода, операции записи в нашей системе могут возвращать клиенту успех до того, как изменения достигнут всех реплик, а чтение может возвращать устаревшее значение. При этом система должна обеспечивать _согласованность в конечном счёте_ (eventual consistency) - изменения должны рано или поздно оказаться на всех живых репликах. Для этого надо как минимум реализовать механизмы hinted handoff и read repair. Реализация фоновой синхронизации (anti-entropy) - по желанию.

Из-за отказов и одновременных операций записи на разных узлах системы может оказаться несколько версий значения ключа. Ваша реализация должна уметь автоматически разрешать возникающие конфликты, используя стратегию _last write wins_. А именно, с каждой операцией записи и соответствующим значением ключа связано время записи, и из двух значений выбирается то, которое было записано позднее. Клиенту всегда возвращается только одно значение ключа. Данная стратегия подразумевает синхронизацию часов между узлами. В своей реализации вы можете опираться на время, доступное через `ctx.time()`. Другим недостатком данной стратегии является возможность потери изменений, сделанных клиентами. Мы устраним эти недостатки в следующем задании (**Внимание! Следующее ДЗ будет требовать наличия готового решения текущей задачи**).

## Реализация

Для реализации и тестирования решения используется фреймворк dslib, см. материалы первого семинара.

В папке задачи размещена заготовка для решения [solution.py](solution.py). Вам надо доработать реализацию узла в классе `StorageNode` так, чтобы проходили все тесты.

При инициализации узлу передается его уникальный id, а также список id всех узлов в системе.

Узел должен поддерживать обработку следующих локальных сообщений (форматы запросов и ответов описаны в заготовке):
- _GET(key, quorum)_ - вернуть значение записи с ключом `key` (может выдать пустое значение, если записи с этим ключом нет),
- _PUT(key, value, quorum)_ - сохранить запись с ключом `key` и значением `value`,
- _DELETE(key, quorum)_ - удалить запись с ключом `key`.

В отличие от предыдущего ДЗ, все операции с хранилищем теперь указывают используемый размер кворума.

Для взаимодействия между узлами вы можете использовать любые собственные типы сообщений.

Для вычисления реплик по ключу используйте функцию `get_key_replicas()` из заготовки (использовать другую функцию не следует, так как это влияет на тесты).

## Тестирование

Перед запуском тестов убедитесь, что на вашей машине [установлен Rust](https://www.rust-lang.org/tools/install) (версия не ниже 1.53), а рядом с папкой задачи есть папка `dslib`.

Тесты находятся в папке `test`. Для запуска тестов перейдите в эту папку и выполните команду: `cargo run --release`. Запустить только один из тестов можно с помощью опции `-t`. По умолчанию вывод тестов не содержит трассы (последовательности событий во время выполнения каждого из тестов), а только финальную сводку. Включить вывод трасс можно с помощью флага `-d`. Все доступные опции можно посмотреть с помощью `cargo run --release -- --help`.

Также можно воспользоваться подготовленным [Docker-образом](Dockerfile) (в нём же тесты запускаются в GitLab CI). Работа с образом полностью аналогична тому, как это описано в первой задаче.

Если вы найдете ошибки или требования из условий, которые не покрывают наши тесты, то вы можете получить за это бонусы. Для этого надо включить в отчёт описание ситуации, которую не ловят тесты, добавив при необходимости пример решения с ошибкой. За это полагается 0.5 балла. Если вы также реализуете тесты, которые ловят найденную проблему, или хотя бы опишите их логику, то получите еще 0.5 балла. Готовые тесты оформляйте как merge request в родительский репозиторий с заданиями.

## Оценивание

Компоненты задачи и их вклад в оценку:
- BASIC - 3
- REPLICAS CHECK - 1
- STALE REPLICA - 1 (read repair)
- DIVERGED REPLICAS - 1 (last write wins)
- SLOPPY QUORUM - 2 (sloppy quorum + hinted handoff)
- PARTITIONED CLIENT - 2 (проверка корректности поведения при разделении сети)
- PARTITIONED CLIENTS - 2 (проверка корректности поведения при разделении сети)
- Описание ситуации, когда при кворумах _W_ + _R_ > _N_ система может выдать не последнее успешно записанное значение - 1 балл (+ 2 балла за воспроизводящий ситуацию тест с комментариями)
- Краткое описание вашего решения в файле `solution.md` с ожидаемой оценкой в баллах и её разбиением на подпункты - обязательно, без него проверка производиться не будет!

Несмотря на то, что за последние три теста баллы разделены, фактически корректная реализация hinted handoff даст вам 6 баллов сразу. За полное решение без дополнительного тест-кейса выставляется 12 баллов, а с ним - 15, таким образом можно получить до 5 бонусных баллов сверх оценки 10.

## Рекомендации

**Начните решать задачу за несколько вечеров до дедлайна, так как можно неожиданно для себя оставить целый вечер на дебаге чего-то, что вы неправильно поняли.**

Мы реализовали полное решение и предлагаем следующий план как решать задачу:

1. Начать стоит с базовой версии репликации с классическими кворумами.

> Не забывайте про дебаг с помощью `cargo run --release -- -t 'TEST NAME' -d`.

2. Имеет смысл сразу учесть случай, когда реплики могут выдать при чтении разные значения, и реализовать last write wins.

> Поскольку взаимодействие между нодами происходит в формате запрос-ответ, лучше сразу сделать какой-то контекст запроса и его передавать, а если очень хочется решить архитектурную задачу — можно написать маленький фреймворк для коммуникации в формате запрос-ответ (с таймаутами, они понадобятся для sloppy quorum) поверх базовой ноды, предложенной в шаблоне.

3. Теперь можно реализовать read repair.
4. К этому моменту у вас скорее всего должны проходить четыре теста до DIVERGED REPLICAS включительно (но это не точно, порядок мы не проверяли). Теперь можно почитать [статью про Dynamo](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf), особенно внимательно читаем раздел 4.6, но и вся глава 4 очень полезна — её подмножество мы фактически реализовываем в этом задании.

> Подумайте, как вы будете это писать в вашей кодовой базе, может быть захочется переписать что-то из уже написанного.

5. После чтения можно реализовать sloppy quorum с помощью таймаутов запросов, должен проходить соответствующий тест.
6. Теперь можно реализовать hinted handoff. С ним должны проходить все тесты.

## Сдача

Следуйте стандартному [порядку сдачи заданий](../README.md).